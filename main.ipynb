{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, column):\n",
    "    # Get the instance of global dictionary of encoders.\n",
    "    global encoders\n",
    "    # Encode given column data.\n",
    "    if column in encoders:\n",
    "        return encoders[column].transform(df[column])\n",
    "    else:\n",
    "        encoders[column] = LabelEncoder()\n",
    "        return encoders[column].fit_transform(df[column])\n",
    "\n",
    "def decode(df, column):\n",
    "    # Get the instance of global dictionary of encoders.\n",
    "    global encoders\n",
    "    # Decode given column data.\n",
    "    if column in encoders:\n",
    "        return encoders[column].inverse_transform(df[column])\n",
    "    else:\n",
    "        raise \"Can't find an appropriate decoder instance.\"\n",
    "        \n",
    "encoders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_d(x):\n",
    "    \"\"\"\n",
    "    Remove 'd_' from a given string and return as interger.\n",
    "    This function will be used for data preprocessing.\n",
    "    \"\"\"\n",
    "    x = x.replace('d_', '')\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) sales_train_validation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load file.\n",
    "df_sales = pd.read_csv('./sales_train_validation.csv')\n",
    "\n",
    "# Create arbitrary values for validation periods.\n",
    "for d in range(1914, 1942):\n",
    "    df_sales[f'd_{d}'] = 0\n",
    "\n",
    "# Encode categorical features.\n",
    "for column in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']:\n",
    "    df_sales[column] = encode(df_sales, column)\n",
    "\n",
    "# Unpivot 'd' data.\n",
    "id_vars = [column for column in df_sales if 'd_' not in column]\n",
    "value_vars = [column for column in df_sales if 'd_' in column]\n",
    "df_sales = df_sales.melt(id_vars=id_vars, value_vars=value_vars, var_name='d', value_name='sales')\n",
    "\n",
    "# Remove 'd_' in multiprocessing manner.\n",
    "with mp.Pool(6) as p:\n",
    "    df_sales['d'] = p.map(remove_d, df_sales['d'])\n",
    "\n",
    "# Reduce memory usage.\n",
    "df_sales = reduce_mem_usage(df_sales)\n",
    "\n",
    "# Sort dataframe w.r.t 'id' and 'd'.\n",
    "df_sales = df_sales.sort_values(['id', 'd'], ignore_index=True)\n",
    "\n",
    "df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) calendar.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load file.\n",
    "df_calendar = pd.read_csv('./calendar.csv')\n",
    "\n",
    "# Create 'day' column.\n",
    "df_calendar['day'] = pd.DatetimeIndex(df_calendar['date']).day\n",
    "\n",
    "# Drop redundant columns.\n",
    "df_calendar.drop(columns=['date', 'weekday'], inplace=True)\n",
    "\n",
    "# Encode categorical features.\n",
    "df_calendar = df_calendar.fillna('')\n",
    "for column in ['wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n",
    "    df_calendar[column] = encode(df_calendar, column)\n",
    "\n",
    "# Remove 'd_' from 'd' column.\n",
    "df_calendar['d'] = df_calendar['d'].apply(remove_d)\n",
    "\n",
    "# Reduce memory usage.\n",
    "df_calendar = reduce_mem_usage(df_calendar)\n",
    "df_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) sell_prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load file.\n",
    "df_prices = pd.read_csv('./sell_prices.csv')\n",
    "\n",
    "# Encode categorical features.\n",
    "for column in ['store_id', 'item_id', 'wm_yr_wk']:\n",
    "    df_prices[column] = encode(df_prices, column)\n",
    "    \n",
    "# Reduce memory usage.\n",
    "df_prices = reduce_mem_usage(df_prices)\n",
    "df_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng():\n",
    "    # Get the global instances.\n",
    "    global df_sales, df_calendar, df_prices\n",
    "    \n",
    "    # Merge dataframes.\n",
    "    df = pd.merge(df_sales, df_calendar, how='left', on='d')\n",
    "    df = pd.merge(df, df_prices, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "    \n",
    "    # Lagged features.\n",
    "    columns = ['sales']\n",
    "    windows = [7, 28, 365]\n",
    "    for window in windows:\n",
    "        for column in columns:\n",
    "            df.loc[:, f'{column}_lag{window}'] = df[column].shift(window)\n",
    "    \n",
    "    # Moving average features.\n",
    "    columns = ['sales_lag7', 'sales_lag28', 'sales_lag365']\n",
    "    windows = [7, 28, 365]\n",
    "    for window in windows:\n",
    "        for column in columns:\n",
    "            df.loc[:, f'{column}_avg{window}'] = df[column].rolling(window).mean()    \n",
    "\n",
    "    # Reduce memory usage.\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Feature engineering.\n",
    "df = feature_eng()\n",
    "df = df.dropna()\n",
    "where = df['d'] <= 1913\n",
    "df = df.loc[where]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Split train/valid dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical/unused features.\n",
    "categorical_feature = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "unused_feature = ['id', 'd', 'wm_yr_wk', 'sales']\n",
    "\n",
    "# Create a dataset.\n",
    "train_data = lgb.Dataset(df.drop(columns=unused_feature), df['sales'], categorical_feature=categorical_feature)\n",
    "\n",
    "# Set parameters.\n",
    "params = {'metric': 'rmse'}\n",
    "\n",
    "# Train a model.\n",
    "model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Iterative inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic = 1.03\n",
    "\n",
    "# Iterate over prediction periods.\n",
    "for d in range(1914, 1942):\n",
    "    \n",
    "    # Keep only last 365 days values.\n",
    "    where = df_sales['d'] >= (d - 730)\n",
    "    df_sales = df_sales[where].copy()\n",
    "    \n",
    "    # Create the most recent data.\n",
    "#     where = df_sales['d'] == df_sales['d'].max()\n",
    "#     df_now = df_sales[where].copy()\n",
    "#     df_now['d'] = df_now['d'] + 1\n",
    "#     df_now['sales'] = 0\n",
    "#     df_sales = pd.concat([df_sales, df_now], ignore_index=True)\n",
    "    \n",
    "    # Feature engineering.\n",
    "    df = feature_eng()\n",
    "    \n",
    "    # Predict the recent values.\n",
    "    where = df['d'] == d\n",
    "    predict = model.predict(df[where].drop(columns=unused_feature))\n",
    "    where = df_sales['d'] == d\n",
    "    df_sales.loc[where, 'sales'] = predict * magic\n",
    "    print(f'The inference for d_{d} is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Brief look on the result to get the intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(600, 700):\n",
    "    df_sales[df_sales.id == i].sales.plot(figsize=[30, 2], use_index=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Save the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indices and columns from sample submission file.\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "indices = df_submission['id']\n",
    "columns = df_submission.columns\n",
    "\n",
    "where = df_sales['d'] > 1913\n",
    "df_submission = df_sales.loc[where, ['id', 'sales', 'd']].copy()\n",
    "df_submission['id'] = decode(df_submission, 'id')\n",
    "for idx, d in zip(range(1, 29), range(1914, 1942)):\n",
    "    where = df_submission['d'] == d\n",
    "    df_submission.loc[where, 'd'] = f'F{idx}'\n",
    "df_submission = df_submission.pivot(index='id', columns='d', values='sales')\n",
    "\n",
    "df_submission = pd.merge(indices, df_submission, how='left', on='id')\n",
    "df_submission = df_submission[columns]\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.fillna(0).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
