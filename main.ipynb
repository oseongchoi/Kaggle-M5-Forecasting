{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, column):\n",
    "    # Get the instance of global dictionary of encoders.\n",
    "    global encoders\n",
    "    # Encode given column data.\n",
    "    if column in encoders:\n",
    "        return encoders[column].transform(df[column])\n",
    "    else:\n",
    "        encoders[column] = LabelEncoder()\n",
    "        return encoders[column].fit_transform(df[column])\n",
    "\n",
    "def decode(df, column):\n",
    "    # Get the instance of global dictionary of encoders.\n",
    "    global encoders\n",
    "    # Decode given column data.\n",
    "    if column in encoder:\n",
    "        return encoders[column].inverse_transform(df[column])\n",
    "    else:\n",
    "        raise \"Can't find an appropriate decoder instance.\"\n",
    "        \n",
    "encoders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) sales_train_validation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files.\n",
    "df_sales = pd.read_csv('./sales_train_validation.csv')\n",
    "\n",
    "# Encode categorical features.\n",
    "for column in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']:\n",
    "    df_sales[column] = encode(df_sales, column)\n",
    "    \n",
    "# Unpivot 'd' data.\n",
    "id_vars = [column for column in df_sales if 'd_' not in column]\n",
    "value_vars = [column for column in df_sales if 'd_' in column]\n",
    "df_sales = df_sales.melt(id_vars=id_vars, value_vars=value_vars, var_name='d', value_name='sales')\n",
    "\n",
    "# Remove 'd_' from 'd' column.\n",
    "df_sales['d'] = df_sales['d'].apply(lambda x: x.replace('d_', '')).astype('int')\n",
    "\n",
    "# Reduce memory usage.\n",
    "df_sales = reduce_mem_usage(df_sales)\n",
    "df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) calendar.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files.\n",
    "df_calendar = pd.read_csv('./calendar.csv')\n",
    "\n",
    "# Create 'day' column.\n",
    "df_calendar['day'] = pd.DatetimeIndex(df_calendar['date']).day\n",
    "\n",
    "# Drop redundant columns.\n",
    "df_calendar.drop(columns=['date', 'weekday'], inplace=True)\n",
    "\n",
    "# Encode categorical features.\n",
    "df_calendar = df_calendar.fillna('')\n",
    "for column in ['wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n",
    "    df_calendar[column] = encode(df_calendar, column)\n",
    "\n",
    "# Remove 'd_' from 'd' column.\n",
    "df_calendar['d'] = df_calendar['d'].apply(lambda x: x.replace('d_', '')).astype('int')\n",
    "\n",
    "# Reduce memory usage.\n",
    "df_calendar = reduce_mem_usage(df_calendar)\n",
    "df_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) sell_prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files.\n",
    "df_prices = pd.read_csv('./sell_prices.csv')\n",
    "\n",
    "# Encode categorical features.\n",
    "for column in ['store_id', 'item_id', 'wm_yr_wk']:\n",
    "    df_prices[column] = encode(df_prices, column)\n",
    "    \n",
    "# Reduce memory usage.\n",
    "df_prices = reduce_mem_usage(df_prices)\n",
    "df_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes.\n",
    "df = pd.merge(df_sales, df_calendar, how='left', on='d')\n",
    "df = pd.merge(df, df_prices, how='left', on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "del df_sales, df_calendar, df_prices\n",
    "\n",
    "# Drop useless columns.\n",
    "df = df.drop(columns='wm_yr_wk')\n",
    "\n",
    "# Arrange column/row orders.\n",
    "columns = [\n",
    "    # Item related features.\n",
    "    'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id',\n",
    "    # Date related features.\n",
    "    'year', 'month', 'day', 'wday', 'd',\n",
    "    # Event and snap related features.\n",
    "    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "    # Sales related features.\n",
    "    'sell_price', 'sales'\n",
    "]\n",
    "df = df[columns]\n",
    "df = df.sort_values(by=['id', 'd'])\n",
    "\n",
    "# Save on disk.\n",
    "df.to_pickle('data.pickle')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df['d'] <= 1183\n",
    "valid = df['d'] > 1183\n",
    "label = 'sales'\n",
    "categorical_feature = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "\n",
    "train_data = lgb.Dataset(df[train].drop(columns=label), df.loc[train, label], categorical_feature=categorical_feature)\n",
    "valid_data = lgb.Dataset(df[valid].drop(columns=label), df.loc[valid, label], categorical_feature=categorical_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'metric': 'mae'\n",
    "}\n",
    "model = lgb.train(params, train_data, valid_sets=[train_data, valid_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
